interface vs data structure
an interface says what you want to do, data stucture says how you do it

interface
- specification
- what data can you store
- what operations can you do, what are supported, what they mean
- problem statement


data stucture
* representation
* how to store data
* implementations of the operations - algorithms to support the operations
* algorithmic solution for the problem

2 main interfaces
- set
- sequences
idea is that we want to store n things
on one hand we care about their values
on the other hand we care about the order of items

2 main data structures
- arrays
- pointers (linked data stuctures)

sequence interface

static sequence interface
maintain a sequence of items x0,x1, .... xn-1 subject to these operaions:
* build(x) - make new DS for items in x
* len(): return
* iter_seq(): output for x0.....xn-1 in sequence order
* get_at(i): return xi (index i)
* set_at(i,x): set xi to x

solution - static array
!! word RAM
idea in word ram- memory is a word of w-bit words 
[w-bit| | | | | | | |]
array - consecutive chunk of memory
array[i] = memory[address(aray) + i] (offset logic)

O(1) -> get_at and set_at, len
O(n) -> build, iter_seq

how to build array in linear time
memory allocation model: allocate an array of size n in Θ(n) time.
space is at most the amount of time you use, or O(time
Space ≤ O(Time)
### why space <= O(time)
---
In the lecture, Erik Demaine makes this observation when discussing the memory allocation model. Let me break down why this relationship between space and time exists:

1. The Memory Allocation Model:
- He assumes that allocating an array of size n takes θ(n) time
- This is a model choice - he mentions you could imagine it being constant time, but this linear-time model is cleaner

2. Why Space ≤ O(Time):
The key insight comes from two facts:
- Every time you allocate space, you must spend time proportional to that space
- The only way to get new space is through allocation (you can't magically get more space without paying the time cost)

So, if we think about it:
- If you want to use S space
- You must have allocated that space at some point
- Each allocation of n units of space costs θ(n) time
- Therefore, the total time T must be at least proportional to the total space S
- This gives us S = O(T)

He mentions this is a "nice feature" because it prevents unrealistic scenarios. For example:
- Without this relationship, you could theoretically allocate an infinitely large array and only use a small part of it
- That would be unrealistic - you shouldn't be able to get "free" space without paying for it in time
- The model ensures that if you want to use space, you must "pay" for it with computational time

This relationship is particularly important for analyzing data structures because it means that space complexity can never be worse than time complexity (asymptotically) in this model. When designing algorithms and data structures, you know that if your algorithm runs in T(n) time, you automatically get an O(T(n)) bound on space usage when working with arrays.

---


for get and set to take constant time - w (word size) >= log2(n)
this is because the memory needs to be addressable. If you word was smaller than log(n), you would not be able to even write down some positionsin you array
like trying to write a 3 digit number when you have space for only 2 digits
biggest number for 64 bit -> 2^18 = 18 quintillion. Even if we assume 1 index takes 1 byte, you need 18 exabytes (18 * 10^18). Modern computers have RAMs in the order of gigabytes (10^8). So physical limits are reached much before addressable limits.

dynamic arrays:
two dynamic operations- 
insert_at(i,x) - make x the new xi, shift others
delete_at(i)  - delete at i and shift others
other special cases:
insert_first / insert_last
delete_first / delete_last
get_first / get_last
set_first / set_last
what makes this interesting from algorithms pov - this could be more efficient only for special cases

solution - linked list
we store our items in a bunch of nodes, each node having the data and the "next" field or pointer which points to the next node. In addition we also need the head (start) of the list, which could also store the length of the list.
we are relying on the fact that pointers are stored in a single word - so they can be derefed and checked

dynamic sequence operations

static array
insert/delete_at() - O(n) time
shifting 
allocation, copying

linked list
insert_first / delete_first - O(1) time
get_at/set_at(i) - O(i) time  - O(n) worst case

goal: best of both worlds

Dynamic arrays in python
- relax constraint of the size of array = n
- roughly n -> Θ(n) & >= n 
- for example 2n
- maintain A[i] = xi
- insert_last

if n == size:
allocate a new array of size n
resize at n=1,2,4,8,16...
resize_cost = Θ(1+2+4+8+16...) = Θ(sigma(1,log(n), 2^i)) = Θ(2n) = Θ(n) [linear time]
- As mentioned in the lecture, for a geometric series like this:
    - The sum of 2ⁱ from i=0 to k equals 2^(k+1) - 1
- In this case, k = log₂(n)
- So plugging in: Σ(2ⁱ) from i=0 to log₂(n) = 2^(log₂(n) + 1) - 1
- Simplify 2^(log₂(n) + 1):
    - 2^(log₂(n) + 1) = 2^log₂(n) × 2¹
    - 2^log₂(n) = n
    - So 2^(log₂(n) + 1) = 2n
- Therefore: Σ(2ⁱ) from i=0 to log₂(n) = 2n - 1

amortization
- For n insert_last operations:
    - Total cost of all resizing = 1 + 2 + 4 + 8 + ... + (≈n)
    - This sums to O(n)
    - Therefore n operations cost O(n) total time
    - So each operation averages to O(1)

4. Definition Given: "An operation takes t(n) amortized time if any k of those operations take at most k×t(n) time"

So while individual insert_last operations might be expensive (when resizing), when you average the cost across all operations, each one effectively costs constant time. This is what we call "constant amortized time" or O(1) amortized.



